<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>硕硕的小窝 | 硕硕的小窝</title><meta name="author" content="Menike Kassel"><meta name="copyright" content="Menike Kassel"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="MADQN推导MADQN（Multi-Agent Deep Q-Network）的推导涉及从基本的Q-learning扩展到多智能体环境中的DQN算法。下面是MADQN推导的具体步骤： 1. 单智能体Q-Learning 回顾Q-learning是强化学习中的一种值函数方法，用来估计在某状态下采取某动作的期望回报（Q值）。其更新公式为： [Q(s_t, a_t) \leftarrow Q(s_t,">
<meta property="og:type" content="article">
<meta property="og:title" content="硕硕的小窝">
<meta property="og:url" content="https://menikekassel.github.io/2024/10/30/MADQN/index.html">
<meta property="og:site_name" content="硕硕的小窝">
<meta property="og:description" content="MADQN推导MADQN（Multi-Agent Deep Q-Network）的推导涉及从基本的Q-learning扩展到多智能体环境中的DQN算法。下面是MADQN推导的具体步骤： 1. 单智能体Q-Learning 回顾Q-learning是强化学习中的一种值函数方法，用来估计在某状态下采取某动作的期望回报（Q值）。其更新公式为： [Q(s_t, a_t) \leftarrow Q(s_t,">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://menikekassel.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2024-10-30T10:18:59.704Z">
<meta property="article:modified_time" content="2024-10-30T02:09:24.000Z">
<meta property="article:author" content="Menike Kassel">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://menikekassel.github.io/img/butterfly-icon.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://menikekassel.github.io/2024/10/30/MADQN/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        if (name && globalFn[key][name]) return
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '硕硕的小窝',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-10-30 10:09:24'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">硕硕的小窝</span></a><a class="nav-page-title" href="/"><span class="site-name">硕硕的小窝</span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">无标题</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-10-30T10:18:59.704Z" title="发表于 2024-10-30 18:18:59">2024-10-30</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-10-30T02:09:24.000Z" title="更新于 2024-10-30 10:09:24">2024-10-30</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="MADQN推导"><a href="#MADQN推导" class="headerlink" title="MADQN推导"></a>MADQN推导</h1><p>MADQN（Multi-Agent Deep Q-Network）的推导涉及从基本的Q-learning扩展到多智能体环境中的DQN算法。下面是MADQN推导的具体步骤：</p>
<h3 id="1-单智能体Q-Learning-回顾"><a href="#1-单智能体Q-Learning-回顾" class="headerlink" title="1. 单智能体Q-Learning 回顾"></a>1. <strong>单智能体Q-Learning 回顾</strong></h3><p>Q-learning是强化学习中的一种值函数方法，用来估计在某状态下采取某动作的期望回报（Q值）。其更新公式为：</p>
<p>[<br>Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma \max_{a’} Q(s_{t+1}, a’) - Q(s_t, a_t) \right]<br>]</p>
<p>其中：</p>
<ul>
<li>( s_t ) 和 ( s_{t+1} ) 是智能体在时间步 ( t ) 和 ( t+1 ) 的状态。</li>
<li>( a_t ) 是时间步 ( t ) 时智能体选择的动作。</li>
<li>( r_{t+1} ) 是智能体在时间步 ( t+1 ) 收到的奖励。</li>
<li>( \alpha ) 是学习率。</li>
<li>( \gamma ) 是折扣因子，用于折扣未来的奖励。</li>
</ul>
<h3 id="2-DQN的引入"><a href="#2-DQN的引入" class="headerlink" title="2. DQN的引入"></a>2. <strong>DQN的引入</strong></h3><p>DQN利用神经网络来近似Q函数，更新公式类似于Q-learning，但使用的是参数化的Q函数 ( Q(s, a; \theta) )，其中 ( \theta ) 是神经网络的参数。DQN的更新公式为：</p>
<p>[<br>\theta \leftarrow \theta + \alpha \left[ r_{t+1} + \gamma \max_{a’} Q(s_{t+1}, a’; \theta^{-}) - Q(s_t, a_t; \theta) \right] \nabla_{\theta} Q(s_t, a_t; \theta)<br>]</p>
<p>其中：</p>
<ul>
<li>( \theta^{-} ) 是目标网络的参数，定期与主网络的参数同步，减少目标值的变化。</li>
<li>( \nabla_{\theta} Q(s_t, a_t; \theta) ) 是Q值对参数的梯度。</li>
</ul>
<h3 id="3-多智能体Q-Learning"><a href="#3-多智能体Q-Learning" class="headerlink" title="3. 多智能体Q-Learning"></a>3. <strong>多智能体Q-Learning</strong></h3><p>在多智能体环境中，有 ( N ) 个智能体，每个智能体 ( i ) 都有自己的状态 ( s^i_t ) 和动作 ( a^i_t )。每个智能体的Q函数可以写为 ( Q^i(s^i_t, a^i_t) )，但由于每个智能体的动作会影响环境和其他智能体的状态，单独考虑某个智能体的Q值可能不足以描述其在环境中的表现。</p>
<h3 id="4-集中式训练，分布式执行"><a href="#4-集中式训练，分布式执行" class="headerlink" title="4. 集中式训练，分布式执行"></a>4. <strong>集中式训练，分布式执行</strong></h3><p>为了有效学习，在训练过程中，MADQN使用集中式学习方法，利用全局信息进行训练。每个智能体的Q值更新公式为：</p>
<p>[<br>Q^i(s^i_t, a^i_t; \theta^i) \leftarrow Q^i(s^i_t, a^i_t; \theta^i) + \alpha \left[ r^i_{t+1} + \gamma \max_{a’^i} Q^i(s^i_{t+1}, a’^i; \theta^{i-}) - Q^i(s^i_t, a^i_t; \theta^i) \right]<br>]</p>
<p>在此处：</p>
<ul>
<li>每个智能体有自己的Q网络 ( Q^i )，参数为 ( \theta^i )。</li>
<li>每个智能体接收的奖励 ( r^i_{t+1} ) 可以是与其本身有关的，也可以是与其他智能体的行动有关的。</li>
</ul>
<h3 id="5-联合动作-值函数的构造"><a href="#5-联合动作-值函数的构造" class="headerlink" title="5. 联合动作-值函数的构造"></a>5. <strong>联合动作-值函数的构造</strong></h3><p>为了应对多智能体之间的依赖性，MADQN可以构建联合的Q值函数：</p>
<p>[<br>Q^{joint}(\mathbf{s}_t, \mathbf{a}<em>t) &#x3D; \sum</em>{i&#x3D;1}^N Q^i(s^i_t, a^i_t)<br>]</p>
<p>其中 ( \mathbf{s}_t &#x3D; (s^1_t, \dots, s^N_t) ) 是所有智能体的联合状态，( \mathbf{a}_t &#x3D; (a^1_t, \dots, a^N_t) ) 是所有智能体的联合动作。</p>
<p>联合Q值函数的更新方式与单智能体类似，但考虑了所有智能体的动作：</p>
<p>[<br>Q^{joint}(\mathbf{s}<em>t, \mathbf{a}t) \leftarrow Q^{joint}(\mathbf{s}t, \mathbf{a}t) + \alpha \left[ \sum{i&#x3D;1}^N r^i{t+1} + \gamma \max{\mathbf{a}’} Q^{joint}(\mathbf{s}</em>{t+1}, \mathbf{a}’) - Q^{joint}(\mathbf{s}_t, \mathbf{a}_t) \right]<br>]</p>
<h3 id="6-分布式执行"><a href="#6-分布式执行" class="headerlink" title="6. 分布式执行"></a>6. <strong>分布式执行</strong></h3><p>在执行过程中，每个智能体只使用它的局部Q值函数 ( Q^i(s^i_t, a^i_t) ) 进行决策，而不考虑全局信息。这使得MADQN可以在去中心化的环境中高效执行。</p>
<h3 id="7-探索与利用"><a href="#7-探索与利用" class="headerlink" title="7. 探索与利用"></a>7. <strong>探索与利用</strong></h3><p>MADQN在多智能体环境中，面对探索与利用的权衡时，通常使用 (\epsilon)-贪婪策略。每个智能体根据自己的局部Q值以 (\epsilon) 的概率随机选择动作，以 (1-\epsilon) 的概率选择最优动作。</p>
<h3 id="8-梯度更新与优化"><a href="#8-梯度更新与优化" class="headerlink" title="8. 梯度更新与优化"></a>8. <strong>梯度更新与优化</strong></h3><p>每个智能体的神经网络参数 ( \theta^i ) 使用反向传播和梯度下降进行更新。联合Q值函数通过所有智能体的联合动作来更新，以确保全局性能的最优化。</p>
<h3 id="9-挑战与改进"><a href="#9-挑战与改进" class="headerlink" title="9. 挑战与改进"></a>9. <strong>挑战与改进</strong></h3><p>在实际应用中，MADQN面临非平稳性（由于其他智能体的策略不断变化，环境对某个智能体来说变得非平稳）的挑战。为解决这一问题，可以引入如下改进：</p>
<ul>
<li><strong>经验回放</strong>：通过记录智能体的经验来缓解非平稳性。</li>
<li><strong>对手建模</strong>：每个智能体可以学习和预测其他智能体的行为。</li>
<li><strong>权重共享</strong>：对于相似任务的智能体，可以共享部分网络权重以加速学习。</li>
</ul>
<p>以上是MADQN算法的详细推导过程，展示了从单智能体Q-learning到多智能体深度Q网络的演化与扩展。MADQN能够处理复杂的多智能体系统，通过协同和对抗优化各智能体的策略，在诸如机器人协作、自动驾驶等领域有着广泛应用。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">graph TD</span><br><span class="line">    A[Start] --&gt; B[Initialize all agents with DQN]</span><br><span class="line">    B --&gt; C[Observe initial state for all agents]</span><br><span class="line">    C --&gt; D[For each agent, select action using epsilon-greedy strategy]</span><br><span class="line">    D --&gt; E[Execute all actions in the environment]</span><br><span class="line">    E --&gt; F[Observe next state and rewards for all agents]</span><br><span class="line">    F --&gt; G[Store experience in replay buffer]</span><br><span class="line">    G --&gt; H[For each agent, sample mini-batch from replay buffer]</span><br><span class="line">    H --&gt; I[For each agent, update Q-network using gradient descent]</span><br><span class="line">    I --&gt; J&#123;Terminate Condition?&#125;</span><br><span class="line">    J -- No --&gt; C</span><br><span class="line">    J -- Yes --&gt; K[End of Training]</span><br><span class="line">    K --&gt; L[Start Execution Phase]</span><br><span class="line">    L --&gt; M[For each agent, observe current state]</span><br><span class="line">    M --&gt; N[For each agent, select action using learned policy]</span><br><span class="line">    N --&gt; O[Execute all actions in the environment]</span><br><span class="line">    O --&gt; P[Observe next state and rewards for all agents]</span><br><span class="line">    P --&gt; Q&#123;Terminate?&#125;</span><br><span class="line">    Q -- No --&gt; M</span><br><span class="line">    Q -- Yes --&gt; R[End]</span><br><span class="line"></span><br></pre></td></tr></table></figure></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://menikekassel.github.io">Menike Kassel</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://menikekassel.github.io/2024/10/30/MADQN/">https://menikekassel.github.io/2024/10/30/MADQN/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://menikekassel.github.io" target="_blank">硕硕的小窝</a>！</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2024/10/30/hello-world/" title="Hello World"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Hello World</div></div><div class="info-2"><div class="info-item-1">Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot;  More info: Writing Run server1$ hexo server  More info: Server Generate static files1$ hexo generate  More info: Generating Deploy to remote sites1$ hexo deploy  More info: Deployment </div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Menike Kassel</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">2</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/MenikeKassel"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#MADQN%E6%8E%A8%E5%AF%BC"><span class="toc-number">1.</span> <span class="toc-text">MADQN推导</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%8D%95%E6%99%BA%E8%83%BD%E4%BD%93Q-Learning-%E5%9B%9E%E9%A1%BE"><span class="toc-number">1.0.1.</span> <span class="toc-text">1. 单智能体Q-Learning 回顾</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-DQN%E7%9A%84%E5%BC%95%E5%85%A5"><span class="toc-number">1.0.2.</span> <span class="toc-text">2. DQN的引入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93Q-Learning"><span class="toc-number">1.0.3.</span> <span class="toc-text">3. 多智能体Q-Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E9%9B%86%E4%B8%AD%E5%BC%8F%E8%AE%AD%E7%BB%83%EF%BC%8C%E5%88%86%E5%B8%83%E5%BC%8F%E6%89%A7%E8%A1%8C"><span class="toc-number">1.0.4.</span> <span class="toc-text">4. 集中式训练，分布式执行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E8%81%94%E5%90%88%E5%8A%A8%E4%BD%9C-%E5%80%BC%E5%87%BD%E6%95%B0%E7%9A%84%E6%9E%84%E9%80%A0"><span class="toc-number">1.0.5.</span> <span class="toc-text">5. 联合动作-值函数的构造</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E5%88%86%E5%B8%83%E5%BC%8F%E6%89%A7%E8%A1%8C"><span class="toc-number">1.0.6.</span> <span class="toc-text">6. 分布式执行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-%E6%8E%A2%E7%B4%A2%E4%B8%8E%E5%88%A9%E7%94%A8"><span class="toc-number">1.0.7.</span> <span class="toc-text">7. 探索与利用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-%E6%A2%AF%E5%BA%A6%E6%9B%B4%E6%96%B0%E4%B8%8E%E4%BC%98%E5%8C%96"><span class="toc-number">1.0.8.</span> <span class="toc-text">8. 梯度更新与优化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-%E6%8C%91%E6%88%98%E4%B8%8E%E6%94%B9%E8%BF%9B"><span class="toc-number">1.0.9.</span> <span class="toc-text">9. 挑战与改进</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/10/30/MADQN/" title="无标题">无标题</a><time datetime="2024-10-30T10:18:59.704Z" title="发表于 2024-10-30 18:18:59">2024-10-30</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/10/30/hello-world/" title="Hello World">Hello World</a><time datetime="2024-10-30T08:47:15.127Z" title="发表于 2024-10-30 16:47:15">2024-10-30</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By Menike Kassel</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>